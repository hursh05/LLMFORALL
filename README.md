# LLMFORALL ğŸš€  
### A Full-Stack Document-Aware Retrieval-Augmented Generation (RAG) System

LLMFORALL is a **production-ready, document-scoped RAG application** that allows users to upload PDF documents and ask natural language questions.

Each query is answered **strictly from the currently uploaded document only**, ensuring **zero context leakage** across documents or user sessions.

---

## ğŸ”¥ Key Features

- ğŸ“„ Upload one or multiple PDF documents  
- ğŸ§  **Document-scoped Question Answering** (No cross-document answers)  
- ğŸ” Semantic search using vector embeddings  
- ğŸ¯ Accurate, context-aware responses via RAG  
- âš¡ FastAPI backend  
- â˜ï¸ Pinecone vector database  
- ğŸ§© Modular, scalable backend architecture  
- ğŸŒ Clean frontend with real-time interaction  

---

## ğŸ§  Core RAG Logic (Important)

LLMFORALL **strictly answers questions ONLY from the selected document**.

### How It Works

1. Each uploaded document is assigned a unique `doc_id`
2. All text chunks are stored in Pinecone with metadata:

```json
{
  "text": "...",
  "doc_id": "blood_cancer"
}
Runtime Flow
When a document is uploaded:

That document becomes the active context

When a question is asked:

Pinecone is queried only with the active doc_id

No vectors from other documents are retrieved

The LLM:

Receives only retrieved document chunks

Is explicitly instructed not to hallucinate or use external knowledge

âœ… Result
If the question is not related to the uploaded document, the system correctly responds:

"No relevant information found in the document."

ğŸ—ï¸ Tech Stack
Backend
Python 3.11

FastAPI

Cohere API â€“ Embeddings

Pinecone â€“ Vector Database

Groq API â€“ LLM Inference

PyPDF2 â€“ PDF Parsing

Frontend
HTML

CSS

JavaScript (Fetch API)

ğŸ“‚ Project Structure
text
Copy code
LLMFORALL/
â”‚
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ main.py                  # FastAPI app entry point
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ upload_pdfs.py        # PDF upload endpoint
â”‚   â”‚   â””â”€â”€ ask_questions.py      # Question answering endpoint
â”‚   â”‚
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ load_vectorstore.py   # PDF â†’ chunks â†’ embeddings â†’ Pinecone
â”‚   â”‚   â”œâ”€â”€ llm.py                # LLM prompt + response logic
â”‚   â”‚   â””â”€â”€ query_handlers.py     # Vector search helpers
â”‚   â”‚
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html            # Frontend UI
â”‚   â”‚
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ style.css
â”‚       â””â”€â”€ script.js
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
ğŸš€ How to Run Locally
1ï¸âƒ£ Clone the Repository
bash
Copy code
git clone https://github.com/your-username/LLMFORALL.git
cd LLMFORALL
2ï¸âƒ£ Create Virtual Environment
bash
Copy code
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
3ï¸âƒ£ Install Dependencies
bash
Copy code
pip install -r requirements.txt
4ï¸âƒ£ Configure Environment Variables
Create a .env file:

env
Copy code
COHERE_API_KEY=your_key
PINECONE_API_KEY=your_key
PINECONE_INDEX_NAME=llmforall
PINECONE_DIMENSION=1024
PINECONE_ENV=aws-region
UPLOAD_DIR=uploaded_docs
5ï¸âƒ£ Run the Server
bash
Copy code
uvicorn server.main:app --reload
Open your browser â†’
ğŸ‘‰ http://127.0.0.1:8000

ğŸ§ª Example Behavior
Scenario	Result
Ask diabetes question on blood cancer PDF	âŒ Correctly says Not Found
Upload second PDF in another session	âœ… No cross-document leakage
Multiple users	âœ… Session-safe
Hallucination	âŒ Prevented

ğŸ›¡ï¸ Design Principles
âŒ No global memory

âŒ No cross-document leakage

âŒ No external knowledge injection

âœ… Deterministic, document-grounded answers

âœ… Production-safe RAG architecture

ğŸ“Œ Use Cases
Medical document Q&A

Legal document analysis

Financial reports

Enterprise knowledge assistants

Internal documentation bots

ğŸ‘¨â€ğŸ’» Author
Hursh Karnik
AI / Backend Engineer
